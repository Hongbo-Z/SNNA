{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import vision_transformer as vits\n",
    "from explanation_generator import Baselines\n",
    "\n",
    "device = torch.cuda.set_device(0) # set device\n",
    "print('GPU:', torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paremeter setting\n",
    "n_last_blocks = 4\n",
    "pretrained_weights = \"../SNNA/ckp/backbone_200.pth\" # pretrained weights for backbone\n",
    "checkpoint_key = \"teacher\"\n",
    "arch = \"vit_small\"\n",
    "patch_size = 8\n",
    "num_labels = 4\n",
    "classifier_weights_dir = \"../SNNA/ckp\" # pretrained weights for linear classifier\n",
    "image_size = 360 # The image short side is resized to 360\n",
    "\n",
    "# construct backbone model\n",
    "backbone = vits.__dict__[\"vit_small\"](patch_size=patch_size, num_classes=0)\n",
    "embed_dim = backbone.embed_dim * n_last_blocks\n",
    "\n",
    "# load backbone weights to evaluate\n",
    "state_dict = torch.load(pretrained_weights, map_location=\"cpu\")\n",
    "if checkpoint_key is not None and checkpoint_key in state_dict:\n",
    "    print(f\"Take key {checkpoint_key} in provided checkpoint dict\")\n",
    "    state_dict = state_dict[checkpoint_key]\n",
    "# remove `module.` prefix\n",
    "state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "# remove `backbone.` prefix induced by multicrop wrapper\n",
    "state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "# load state dict for backbone\n",
    "msg = backbone.load_state_dict(state_dict, strict=False)\n",
    "print('Pretrained weights found at {} and loaded with msg: {}'.format(pretrained_weights, msg))\n",
    "print(f\"Model {arch} built.\")\n",
    "\n",
    "# construct classifier\n",
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "    def __init__(self, dim, num_labels=1000):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.linear = nn.Linear(dim, num_labels)\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # linear layer\n",
    "        return self.linear(x)\n",
    "\n",
    "# construct a linear classifier head\n",
    "linear_classifier = LinearClassifier(embed_dim, num_labels)\n",
    "# load pretrained weights for linear classifier\n",
    "state_dict = torch.load(os.path.join(classifier_weights_dir, \"classifier.pth.tar\"), map_location=\"cpu\")[\"state_dict\"]\n",
    "# remove `module.` prefix\n",
    "state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "# load state dict for linear classifier\n",
    "linear_classifier.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "# construct the model with the backbone and the linear classifier\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, backbone, head):\n",
    "        super(Model, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, x, register_hook=None):\n",
    "        x = x.unsqueeze(0).cuda() # (1, 3, w, h) add a batch dimension\n",
    "        intermediate_output = self.backbone.get_intermediate_layers(x, n_last_blocks, register_hook=register_hook)\n",
    "        output = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
    "        logits = self.head(output)\n",
    "        return logits\n",
    "    \n",
    "model = Model(backbone, linear_classifier)\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image \n",
    "input_image  = Image.open('../SNNA/data/1.jpg')\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis('off')\n",
    "plt.imshow(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ preparing data ... ============\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size, interpolation=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "image = transform(input_image)\n",
    "# make the image divisible by the patch size\n",
    "w, h = image.shape[1] - image.shape[1] % patch_size, image.shape[2] - image.shape[2] % patch_size\n",
    "img = image[:, :w, :h] \n",
    "print(f\"Image shape: {img.shape}\") \n",
    "\n",
    "\n",
    "# ============ forward ... ============\n",
    "output = model(img)\n",
    "print(torch.nn.functional.softmax(output, dim=-1))\n",
    "\n",
    "w_featmap = img.shape[-2] // patch_size\n",
    "h_featmap = img.shape[-1] // patch_size\n",
    "\n",
    "def show_cam_on_image(img, mask):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET) # chage a grayscale image to a color map\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    # cam = cam / np.max(cam)  # scale the value to [0, 1]\n",
    "    cam = (cam-np.min(cam))/(np.max(cam)-np.min(cam)) # min-max normalization the cam value to [0, 1]\n",
    "    return cam\n",
    "\n",
    "# ============ generate attribution maps ... ============\n",
    "# compare all methods in a row\n",
    "attribution_generator = Baselines(model)\n",
    "batch_size = 1\n",
    "\n",
    "def generate_attribution(image, class_index=None):\n",
    "    rawAttn = attribution_generator.rawAttn(image).detach()\n",
    "    att_gradient = attribution_generator.att_gradient(image, index=class_index).detach()\n",
    "    generic_att = attribution_generator.generic_att(image, index=class_index).detach()\n",
    "    norm_att = attribution_generator.norm_att(image, index=class_index).detach()\n",
    "    IGradient = attribution_generator.IGradient(image, index=class_index, steps=20).detach()\n",
    "    SNNA = attribution_generator.SGradient_normAtt(image, index=class_index, magnitude=False).detach()\n",
    "\n",
    "    return rawAttn, att_gradient, generic_att, norm_att, IGradient, SNNA\n",
    "\n",
    "\n",
    "def generate_visualization(attribution, image):\n",
    "    attribution = attribution.reshape(batch_size, 1, w_featmap, h_featmap)\n",
    "    attribution = torch.nn.functional.interpolate(attribution, scale_factor=patch_size, mode='bilinear')\n",
    "    attribution = attribution.reshape(w, h).data.cpu().numpy()\n",
    "    attribution = (attribution - attribution.min()) / (attribution.max() - attribution.min()) # min-max normalization the attribution value to [0, 1]\n",
    "\n",
    "    vis = show_cam_on_image(image, attribution)\n",
    "    vis =  np.uint8(255 * vis)\n",
    "    vis = cv2.cvtColor(vis, cv2.COLOR_RGB2BGR)\n",
    "    return vis\n",
    "\n",
    "rawAttn, att_gradient, generic_att, norm_att, IGradient, SNNA = generate_attribution(img) # cam: class activation map overlay on image\n",
    "\n",
    "img = img.permute(1, 2, 0).data.cpu().numpy() # (3, w, h)->(w, h, 3)\n",
    "img = (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "rawAttn_vis = generate_visualization(rawAttn, img)\n",
    "att_gradient_vis = generate_visualization(att_gradient, img)\n",
    "generic_att_vis = generate_visualization(generic_att, img)\n",
    "norm_att_vis = generate_visualization(norm_att, img)\n",
    "IGradient_vis = generate_visualization(IGradient, img)\n",
    "SNNA = generate_visualization(SNNA, img)\n",
    "\n",
    "vis_list = [rawAttn_vis, att_gradient_vis, generic_att_vis, norm_att_vis, IGradient_vis, SNNA]\n",
    "\n",
    "# plot baseline figure\n",
    "fig, axs = plt.subplots(1, 6, figsize=(102, 10))\n",
    "# remove white space between subplots\n",
    "plt.subplots_adjust(wspace=0.01, hspace=0)\n",
    "for ax, vis, in zip(axs, vis_list):\n",
    "    ax.axis('off')\n",
    "    ax.imshow(vis)\n",
    "# plt.show()\n",
    "# save the figure\n",
    "# plt.savefig('../SNNA/output/campare.jpg', bbox_inches='tight', pad_inches=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
